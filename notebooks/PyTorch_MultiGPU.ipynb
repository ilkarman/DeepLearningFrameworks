{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from common.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark=True # enables cudnn's auto-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "PyTorch:  0.3.1\n",
      "Numpy:  1.14.1\n",
      "GPU:  ['Tesla P100-PCIE-16GB', 'Tesla P100-PCIE-16GB']\n",
      "CUDA Version 8.0.61\n",
      "CuDNN Version  6.0.21\n"
     ]
    }
   ],
   "source": [
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"GPU: \", get_gpu_name())\n",
    "print(get_cuda_version())\n",
    "print(\"CuDNN Version \", get_cudnn_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "PyTorch:  0.3.1\n",
      "CPUs:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "print(\"CPUs: \", CPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-set\n",
    "# Note if NUM_GPUS > 1 then MULTI_GPU = True and ALL GPUs will be used\n",
    "# Set below to affect batch-size\n",
    "# E.g. 1 GPU = 64, 2 GPUs = 64*2, 4 GPUs = 64*4\n",
    "# Note that the effective learning-rate will be decreased this way\n",
    "NUM_GPUS = 2 # Scaling factor for batch\n",
    "MULTI_GPU=NUM_GPUS>1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "CLASSES = 14\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "LR = 0.0001  # Effective learning-rate will decrease as BATCHSIZE rises\n",
    "EPOCHS = 5\n",
    "BATCHSIZE = 64*NUM_GPUS\n",
    "IMAGENET_RGB_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_RGB_SD = [0.229, 0.224, 0.225]\n",
    "TOT_PATIENT_NUMBER = 30805  # From data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestxray/images chestxray/Data_Entry_2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "CSV_DEST = \"chestxray\"\n",
    "IMAGE_FOLDER = os.path.join(CSV_DEST, \"images\")\n",
    "LABEL_FILE = os.path.join(CSV_DEST, \"Data_Entry_2017.csv\")\n",
    "print(IMAGE_FOLDER, LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please make sure to download\n",
      "https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\n",
      "Data already exists\n",
      "CPU times: user 726 ms, sys: 175 ms, total: 901 ms\n",
      "Wall time: 901 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Download data\n",
    "print(\"Please make sure to download\")\n",
    "print(\"https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\")\n",
    "download_data_chextxray(CSV_DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayData(Dataset):\n",
    "    def __init__(self, img_dir, lbl_file, patient_ids, transform=None):\n",
    "        # Read labels-csv\n",
    "        df = pd.read_csv(lbl_file)\n",
    "        \n",
    "        # Split labels on unfiltered data\n",
    "        df_label = df['Finding Labels'].str.split(\n",
    "            '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "        \n",
    "        # Filter by patient-ids (both)\n",
    "        df_label['Patient ID'] = df['Patient ID']\n",
    "        df_label = df_label[df_label['Patient ID'].isin(patient_ids)]\n",
    "        df = df[df['Patient ID'].isin(patient_ids)]\n",
    "        \n",
    "        # Remove unncessary columns\n",
    "        df_label.drop(['Patient ID','No Finding'], axis=1, inplace=True)  \n",
    "        \n",
    "        # List of images (full-path)\n",
    "        self.img_locs =  df['Image Index'].map(lambda im: os.path.join(img_dir, im)).values\n",
    "        # One-hot encoded labels (float32 for BCE loss)\n",
    "        self.labels = df_label.values\n",
    "        \n",
    "        # Processing\n",
    "        self.transform = transform\n",
    "        print(\"Loaded {} labels and {} images\".format(len(self.labels), len(self.img_locs)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        im_file = self.img_locs[idx]\n",
    "        im_rgb = Image.open(im_file).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform is not None:\n",
    "            im_rgb = self.transform(im_rgb)\n",
    "        return im_rgb, torch.FloatTensor(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_augmentation_dataset(img_dir, lbl_file, patient_ids, normalize):\n",
    "    dataset = XrayData(img_dir, lbl_file, patient_ids,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize(WIDTH),\n",
    "                           transforms.ToTensor(),  \n",
    "                           normalize]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:21563 valid:3080 test:6162\n"
     ]
    }
   ],
   "source": [
    "# Training / Valid / Test split (70% / 10% / 20%)\n",
    "train_set, other_set = train_test_split(\n",
    "    range(1,TOT_PATIENT_NUMBER+1), train_size=0.7, test_size=0.3, shuffle=False)\n",
    "valid_set, test_set = train_test_split(other_set, train_size=1/3, test_size=2/3, shuffle=False)\n",
    "print(\"train:{} valid:{} test:{}\".format(\n",
    "    len(train_set), len(valid_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87306 labels and 87306 images\n"
     ]
    }
   ],
   "source": [
    "# Normalise by imagenet mean/sd\n",
    "normalize = transforms.Normalize(IMAGENET_RGB_MEAN, IMAGENET_RGB_SD)\n",
    "# Dataset for training\n",
    "train_dataset = XrayData(img_dir=IMAGE_FOLDER,\n",
    "                         lbl_file=LABEL_FILE,\n",
    "                         patient_ids=train_set,\n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.Resize(WIDTH+40),\n",
    "                             transforms.RandomHorizontalFlip(),\n",
    "                             transforms.RandomResizedCrop(size=WIDTH),\n",
    "                             transforms.RandomRotation(10),\n",
    "                             transforms.ToTensor(),  # need to convert image to tensor!\n",
    "                             normalize]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7616 labels and 7616 images\n",
      "Loaded 17198 labels and 17198 images\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = no_augmentation_dataset(IMAGE_FOLDER, LABEL_FILE, valid_set, normalize)\n",
    "test_dataset = no_augmentation_dataset(IMAGE_FOLDER, LABEL_FILE, test_set, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol(model_name='densenet121', out_features=CLASSES, multi_gpu=MULTI_GPU):\n",
    "    if model_name == 'densenet121':\n",
    "        model = models.densenet.densenet121(pretrained=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model-name\")\n",
    "    # Replace classifier (FC-1000) with (FC-14)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(model.classifier.in_features, out_features), \n",
    "        nn.Sigmoid())\n",
    "    if multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "    # CUDA\n",
    "    model.cuda()  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_symbol(sym, lr=LR):\n",
    "    opt = optim.Adam(sym.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    # BCE Loss since classes not mutually exclusive + Sigmoid FC-layer\n",
    "    cri = nn.BCELoss()\n",
    "    sch = ReduceLROnPlateau(opt, factor=0.1, patience=5, mode='min')\n",
    "    return opt, cri, sch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_auc(data_gt, data_pd, full=True, classes=CLASSES):\n",
    "    roc_auc = []\n",
    "    data_gt = data_gt.cpu().numpy()\n",
    "    data_pd = data_pd.cpu().numpy()\n",
    "    for i in range(classes):\n",
    "        roc_auc.append(roc_auc_score(data_gt[:, i], data_pd[:, i]))\n",
    "    print(\"Full AUC\", roc_auc)\n",
    "    roc_auc = np.mean(roc_auc)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, epoch, batch=BATCHSIZE):\n",
    "    model.train()\n",
    "    print(\"Training epoch {}\".format(epoch+1))\n",
    "    loss_val = 0\n",
    "    loss_cnt = 0\n",
    "    for data, target in dataloader:\n",
    "        # Get samples\n",
    "        data = Variable(torch.FloatTensor(data).cuda())\n",
    "        target = Variable(torch.FloatTensor(target).cuda())\n",
    "        # Init\n",
    "        optimizer.zero_grad()\n",
    "        # Forwards\n",
    "        output = model(data)\n",
    "        # Loss\n",
    "        loss = criterion(output, target)\n",
    "        # Back-prop\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "         # Log the loss\n",
    "        loss_val += loss.data[0]\n",
    "        loss_cnt += 1\n",
    "    print(\"Training loss: {0:.4f}\".format(loss_val/loss_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, dataloader, criterion, epoch, phase='valid', batch=BATCHSIZE):\n",
    "    model.eval()\n",
    "    if phase == 'testing':\n",
    "        print(\"Testing epoch {}\".format(epoch+1))\n",
    "    else:\n",
    "        print(\"Validating epoch {}\".format(epoch+1))\n",
    "    out_pred = torch.FloatTensor().cuda()\n",
    "    out_gt = torch.FloatTensor().cuda()\n",
    "    loss_val = 0\n",
    "    loss_cnt = 0\n",
    "    for data, target in dataloader:\n",
    "        # Get samples\n",
    "        data = Variable(torch.FloatTensor(data).cuda(), volatile=True)\n",
    "        target = Variable(torch.FloatTensor(target).cuda(), volatile=True)\n",
    "         # Forwards\n",
    "        output = model(data)\n",
    "        # Loss\n",
    "        loss = criterion(output, target)\n",
    "        # Log the loss\n",
    "        loss_val += loss.data[0]\n",
    "        loss_cnt += 1\n",
    "        # Log for AUC\n",
    "        out_pred = torch.cat((out_pred, output.data), 0)\n",
    "        out_gt = torch.cat((out_gt, target.data), 0)\n",
    "    loss_mean = loss_val/loss_cnt\n",
    "    if phase == 'testing':\n",
    "        print(\"Test-Dataset loss: {0:.4f}\".format(loss_mean))\n",
    "        print(\"Test-Dataset AUC: {0:.4f}\".format(compute_roc_auc(out_gt, out_pred)))\n",
    "    else:\n",
    "        print(\"Validation loss: {0:.4f}\".format(loss_mean))\n",
    "        print(\"Validation AUC: {0:.4f}\".format(compute_roc_auc(out_gt, out_pred)))\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def print_learning_rate(opt):\n",
    "#    for param_group in opt.param_groups:\n",
    "#        print(\"Learning rate: \", param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "# https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5\n",
    "# Only multi-threading not multi-processing?\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCHSIZE,\n",
    "                          shuffle=True, num_workers=4*CPU_COUNT, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=16*BATCHSIZE,\n",
    "                          shuffle=False, num_workers=4*CPU_COUNT, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16*BATCHSIZE,\n",
    "                         shuffle=False, num_workers=4*CPU_COUNT, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Train CheXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 s, sys: 842 ms, total: 3.04 s\n",
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load symbol\n",
    "chexnet_sym = get_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 ms, sys: 0 ns, total: 2.08 ms\n",
      "Wall time: 2.09 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load optimiser, loss\n",
    "optimizer, criterion, scheduler = init_symbol(chexnet_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "Training loss: 0.1835\n",
      "Validating epoch 1\n",
      "Validation loss: 0.1399\n",
      "Full AUC [0.7783534395149987, 0.8502879925903414, 0.7898415391216569, 0.9117361631705505, 0.8988459834939467, 0.8635887920256611, 0.7571184203332109, 0.7646857743886404, 0.6446231344471807, 0.7722986472214561, 0.6901967590420974, 0.7761493194059881, 0.7871873923552003, 0.8586092375224146]\n",
      "Validation AUC: 0.7960\n",
      "Epoch time: 380 seconds\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Training epoch 2\n",
      "Training loss: 0.1595\n",
      "Validating epoch 2\n",
      "Validation loss: 0.1369\n",
      "Full AUC [0.7905991554527615, 0.8723404101361815, 0.7857609668866617, 0.9182412420105919, 0.9007426731723862, 0.9017025236610906, 0.7696235950229249, 0.7861688140941363, 0.653542338937953, 0.7889835886189909, 0.7111402455422756, 0.7886741450720958, 0.7868186307510016, 0.869697539153081]\n",
      "Validation AUC: 0.8089\n",
      "Epoch time: 339 seconds\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Training epoch 3\n",
      "Training loss: 0.1561\n",
      "Validating epoch 3\n",
      "Validation loss: 0.1356\n",
      "Full AUC [0.7954471982758622, 0.8924571990911592, 0.7954065194907622, 0.92413124551179, 0.9047593145360254, 0.8799082973631955, 0.7869215087442493, 0.8338285564028398, 0.656445821126302, 0.8176908363996762, 0.6781316646965746, 0.7988332382861332, 0.8067803538682091, 0.8701144134639898]\n",
      "Validation AUC: 0.8172\n",
      "Epoch time: 338 seconds\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Training epoch 4\n",
      "Training loss: 0.1544\n",
      "Validating epoch 4\n",
      "Validation loss: 0.1339\n",
      "Full AUC [0.8051681428721615, 0.8781494667071883, 0.794248259509761, 0.9231702560080243, 0.9071897582726467, 0.9003872329111203, 0.794423051387598, 0.8673810149881672, 0.6686992921691011, 0.8191326341847341, 0.7192043329518234, 0.791616276361583, 0.8046664065325118, 0.8808052671662814]\n",
      "Validation AUC: 0.8253\n",
      "Epoch time: 338 seconds\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Training epoch 5\n",
      "Training loss: 0.1529\n",
      "Validating epoch 5\n",
      "Validation loss: 0.1348\n",
      "Full AUC [0.7852515243902438, 0.8718794772717404, 0.7965309865547593, 0.9115458578093014, 0.9061195439501409, 0.9089931944310872, 0.7943590023979318, 0.9129897449382066, 0.6701309646385304, 0.8180061035358801, 0.6942094927169133, 0.8035020179055674, 0.8110439812531892, 0.8796793298928812]\n",
      "Validation AUC: 0.8260\n",
      "Epoch time: 338 seconds\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "CPU times: user 22min 4s, sys: 3min 43s, total: 25min 48s\n",
      "Wall time: 28min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training loop: 28m53s\n",
    "loss_min = float(\"inf\")    \n",
    "# Main train/val loop\n",
    "for j in range(EPOCHS):\n",
    "    stime = time.time()\n",
    "    train_epoch(chexnet_sym, train_loader, optimizer, criterion, j)\n",
    "    loss_val = valid_epoch(chexnet_sym, valid_loader, criterion, j)\n",
    "    # LR Schedule\n",
    "    scheduler.step(loss_val)\n",
    "    #print_learning_rate(optimizer)\n",
    "    \n",
    "    # I comment this out to create a fair test against Keras\n",
    "    # Keras cannot checkpoint multi-gpu models at the moment\n",
    "    \n",
    "    #if loss_val < loss_min:\n",
    "    #    print(\"Loss decreased. Saving ...\")\n",
    "    #    loss_min = loss_val\n",
    "    #    torch.save({'epoch': j + 1, \n",
    "    #                'state_dict': chexnet_sym.state_dict(), \n",
    "    #                'best_loss': loss_min, \n",
    "    #                'optimizer' : optimizer.state_dict()}, 'best_chexnet.pth.tar')\n",
    "    \n",
    "    etime = time.time()\n",
    "    print(\"Epoch time: {0:.0f} seconds\".format(etime-stime))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Test CheXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 10.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load model for testing\n",
    "# I comment this out to create a fair test against Keras\n",
    "#chexnet_sym_test = get_symbol()\n",
    "#chkpt = torch.load(\"best_chexnet.pth.tar\")\n",
    "#chexnet_sym_test.load_state_dict(chkpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing epoch 0\n",
      "Test-Dataset loss: 0.1540\n",
      "Full AUC [0.8002874678587735, 0.8327232954883196, 0.7960755212055248, 0.886545471239617, 0.8805040798213019, 0.923330137938897, 0.7402972652710587, 0.8543066530487914, 0.6278403229432927, 0.8110967774108574, 0.7306166324926078, 0.7963945586737139, 0.7731040564373898, 0.8799044890256512]\n",
      "Test-Dataset AUC: 0.8095\n",
      "CPU times: user 11.4 s, sys: 10.6 s, total: 22 s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Evaluate\n",
    "# AUC: 0.8095\n",
    "#test_loss = valid_epoch(chexnet_sym_test, test_loader, criterion, -1, 'testing')\n",
    "test_loss = valid_epoch(chexnet_sym, test_loader, criterion, -1, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Extra: IO Experiment (time on numpy arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen_to_numpy(data_gen, bs, ch=CHANNELS, wi=WIDTH, hi=HEIGHT, cl=CLASSES):\n",
    "    x_dta = np.zeros((data_gen.__len__()*bs, ch, wi, hi), \n",
    "                   dtype=np.float32)\n",
    "    y_dta = np.zeros((data_gen.__len__()*bs, cl),\n",
    "                   dtype=np.int32)\n",
    "    c = 0\n",
    "    for x, y in data_gen:\n",
    "        ln = len(y)\n",
    "        x_dta[c*ln:(c+1)*ln] = x\n",
    "        y_dta[c*ln:(c+1)*ln] = y\n",
    "        c+= 1\n",
    "    return x_dta, y_dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.7 s, sys: 17.3 s, total: 39 s\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train = data_gen_to_numpy(train_loader, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 s, sys: 5.08 s, total: 6.86 s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_val, y_val = data_gen_to_numpy(valid_loader, 16*BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87424, 3, 224, 224) (87424, 14)\n",
      "(8192, 3, 224, 224) (8192, 14)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = yield_mb(x_train, y_train, BATCHSIZE, shuffle=False)\n",
    "valid_loader = yield_mb(x_val, y_val, BATCHSIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "Training loss: 0.1426\n",
      "Validating epoch 0\n",
      "Validation loss: 0.1181\n",
      "Full AUC [0.7325276556792604, 0.8143018460329929, 0.6344205025402343, 0.7513804306120363, 0.8940063333242, 0.9349639412633591, 0.8541304263464525, 0.9589464678562699, 0.5537374593754416, 0.8553994709891826, 0.7931636873466525, 0.7896508852649442, 0.7662702585788976, 0.8515950708399794]\n",
      "Validation AUC: 0.7989\n",
      "Epoch time: 280 seconds\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "CPU times: user 4min 37s, sys: 34.3 s, total: 5min 12s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Time for one epoch is 280s vs 340s with data-generator (60s of IO lag)\n",
    "stime = time.time()\n",
    "train_epoch(chexnet_sym, train_loader, optimizer, criterion, -1)\n",
    "loss_val = valid_epoch(chexnet_sym, valid_loader, criterion, -1)\n",
    "scheduler.step(loss_val)\n",
    "etime = time.time()\n",
    "print(\"Epoch time: {0:.0f} seconds\".format(etime-stime))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
